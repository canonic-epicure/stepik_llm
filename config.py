vocab_size = 2000
emb_size = 512

seq_len = 32
max_seq_len = 128

batch_size = 32

num_heads = 12

head_size = 64

num_layers = 6

dropout = 0.1

learning_rate = 0.00025

train_ratio = 0.9

num_epoch = 100


# vocab_size = 2000
# emb_size = 512
#
# seq_len = 64
# max_seq_len = 256
#
# batch_size = 32
#
# num_heads = 8
#
# head_size = 48
#
# num_layers = 12
#
# dropout = 0.1
#
# learning_rate = 0.005
#
# train_ratio = 0.9
#
# num_epoch = 100